{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fastai==1.0.61"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sIl7sZyP41ut",
        "outputId": "7ea20eb3-b0ba-4713-d1a0-ba8335c7ac03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastai==1.0.61\n",
            "  Downloading fastai-1.0.61-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.2/239.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bottleneck\n",
            "  Downloading Bottleneck-1.3.7-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (353 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.1/353.1 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numexpr in /usr/local/lib/python3.9/dist-packages (from fastai==1.0.61) (2.8.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from fastai==1.0.61) (6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from fastai==1.0.61) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from fastai==1.0.61) (1.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from fastai==1.0.61) (23.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from fastai==1.0.61) (2.0.0+cu118)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fastai==1.0.61) (2.27.1)\n",
            "Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from fastai==1.0.61) (1.0.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from fastai==1.0.61) (1.5.3)\n",
            "Collecting nvidia-ml-py3\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from fastai==1.0.61) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.9/dist-packages (from fastai==1.0.61) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from fastai==1.0.61) (8.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from fastai==1.0.61) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->fastai==1.0.61) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->fastai==1.0.61) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->fastai==1.0.61) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->fastai==1.0.61) (3.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->fastai==1.0.61) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->fastai==1.0.61) (3.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.0.0->fastai==1.0.61) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.0.0->fastai==1.0.61) (16.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->fastai==1.0.61) (2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->fastai==1.0.61) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->fastai==1.0.61) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->fastai==1.0.61) (1.0.7)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->fastai==1.0.61) (5.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->fastai==1.0.61) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->fastai==1.0.61) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->fastai==1.0.61) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->fastai==1.0.61) (2022.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fastai==1.0.61) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fastai==1.0.61) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->fastai==1.0.61) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fastai==1.0.61) (2022.12.7)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->fastai==1.0.61) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->fastai==1.0.61) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.0.0->fastai==1.0.61) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.0.0->fastai==1.0.61) (1.3.0)\n",
            "Building wheels for collected packages: nvidia-ml-py3\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19188 sha256=cdccba8994f083c7dcbbf17b1eca0af39d89ce805ea8127b0641f78fa1457d36\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/d8/b0/15cfd7805d39250ac29318105f09b1750683387630d68423e1\n",
            "Successfully built nvidia-ml-py3\n",
            "Installing collected packages: nvidia-ml-py3, bottleneck, fastai\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 2.7.12\n",
            "    Uninstalling fastai-2.7.12:\n",
            "      Successfully uninstalled fastai-2.7.12\n",
            "Successfully installed bottleneck-1.3.7 fastai-1.0.61 nvidia-ml-py3-7.352.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "fastai"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "bDXa2BZYrCWr",
        "outputId": "fd5a10e0-ff50-4ef4-b2dd-653279001692"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-469335c68e5b>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSaveModelCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataBunch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeviceDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_train\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/callbacks/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlr_finder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mone_cycle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneral_sched\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/callbacks/lr_finder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"Tools to help find the optimal learning rate for training\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_core\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_train\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLearnerCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mDatasetType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DatasetType'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train Valid Test Single Fix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'DataBunch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DeviceDataLoader'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DatasetType'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'load_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Enum' is not defined"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from fastai import *\n",
        "from fastai.callbacks import SaveModelCallback\n",
        "from fastai.basic_data import DataBunch, DeviceDataLoader, DatasetType\n",
        "from fastai.basic_train import Learner\n",
        "from fastai.train import *\n",
        "from fastai.distributed import *\n",
        "\n",
        "#from moldataset import MoleculeDataset, collate_parallel_fn\n",
        "#from model import Transformer\n",
        "import io, os, sys, types\n",
        "# import utils, callbacks, losses_and_metrics, constants\n",
        "# from utils import scale_features, set_seed, store_submit, store_oof\n",
        "# from callbacks import GradientClipping, GroupMeanLogMAE\n",
        "# from losses_and_metrics import rmse, mae, contribs_rmse_loss\n",
        "# import constants as C\n",
        "import os\n",
        "\n",
        "import math\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from fcnet import FullyConnectedNet, hidden_layer\n",
        "# from scatter import scatter_mean\n",
        "# from layernorm import LayerNorm\n",
        "\n",
        "device = torch.device(\"cpu\",1)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoYr9ueA9Pu1",
        "outputId": "15785e89-bb26-463f-be19-35ae62f5bde8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2022.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rdkit) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from rdkit) (8.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2022.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from time import strftime, localtime\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from rdkit import Chem, RDConfig\n",
        "\n",
        "TYPES = np.array(['1JHC', '1JHN', '2JHC', '2JHN', '2JHH', '3JHC', '3JHN', '3JHH'])\n",
        "TYPES_MAP = {t: i for i, t in enumerate(TYPES)}\n",
        "SYMBOLS = ['H', 'C', 'N', 'O', 'F']\n",
        "DEGREES = [1, 2, 3, 4, 5]\n",
        "HYBRIDIZATIONS = [Chem.rdchem.HybridizationType.SP, \n",
        "                  Chem.rdchem.HybridizationType.SP2, \n",
        "                  Chem.rdchem.HybridizationType.SP3,\n",
        "                  Chem.rdchem.HybridizationType.UNSPECIFIED]\n",
        "ATOMIC_RADIUS = {'H': 0.38, 'C': 0.77, 'N': 0.75, 'O': 0.73, 'F': 0.71}\n",
        "ELECTRO_NEG = {'H': 2.2, 'C': 2.55, 'N': 3.04, 'O': 3.44, 'F': 3.98}\n",
        "\n",
        "# feature definition file\n",
        "FDEF = os.path.join(RDConfig.RDDataDir, 'BaseFeatures.fdef')\n",
        "\n",
        "SC_EDGE_FEATS = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5', \n",
        "                 'type_6', 'type_7', 'dist', 'dist_min_rad', \n",
        "                 'dist_electro_neg_adj', 'normed_dist', 'diangle', 'cos_angle', 'cos_angle0', 'cos_angle1']\n",
        "SC_MOL_FEATS = ['type_0', 'type_1', 'type_2', 'type_3', 'type_4', 'type_5',\n",
        "                 'type_6', 'type_7', 'dist', 'dist_min_rad', \n",
        "                 'dist_electro_neg_adj', 'normed_dist', 'diangle', 'cos_angle', \n",
        "                 'cos_angle0', 'cos_angle1', 'num_atoms', 'num_C_atoms', \n",
        "                 'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms', \n",
        "                 'std_bond_length', 'ave_bond_length', 'ave_atom_weight']\n",
        "ATOM_FEATS    = ['type_H', 'type_C', 'type_N', 'type_O', 'type_F', 'degree_1', \n",
        "                 'degree_2', 'degree_3', 'degree_4', 'degree_5', 'SP', 'SP2', \n",
        "                 'SP3', 'hybridization_unspecified', 'aromatic', \n",
        "                 'formal_charge', 'atomic_num', 'ave_bond_length', 'ave_neighbor_weight']\n",
        "BOND_FEATS    = ['single', 'double', 'triple', 'aromatic', 'conjugated', 'in_ring', 'dist', 'normed_dist']\n",
        "\n",
        "TARGET_COL   = 'scalar_coupling_constant'\n",
        "\n",
        "N_TYPES            = 8\n",
        "N_SC_EDGE_FEATURES = 16\n",
        "#N_SC_EDGE_FEATURES = 12\n",
        "N_SC_MOL_FEATURES  = 25\n",
        "#N_SC_MOL_FEATURES  = 21\n",
        "N_ATOM_FEATURES    = 19\n",
        "N_BOND_FEATURES    = 8\n",
        "MAX_N_ATOMS        = 29\n",
        "MAX_N_SC           = 135\n",
        "BATCH_PAD_VAL      = -999\n",
        "\n",
        "N_SC               = 4658147\n",
        "N_SC_TRAIN         = 3762130\n",
        "N_SC_VAL           = 536619\n",
        "N_SC_TEST          = 359398\n",
        "\n",
        "N_MOLS             = 85003\n",
        "N_TRAIN_MOLS       = 66951\n",
        "N_VAL_MOLS         = 9552\n",
        "N_TEST_MOLS        = 8500\n",
        "\n",
        "N_FOLDS = 8\n",
        "\n",
        "SC_MEAN = 16\n",
        "SC_STD = 35\n",
        "SC_FEATS_TO_SCALE = ['dist', 'dist_min_rad', 'dist_electro_neg_adj', 'num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms', \n",
        "                       'num_N_atoms', 'num_O_atoms', 'ave_bond_length', 'std_bond_length', 'ave_atom_weight']\n",
        "ATOM_FEATS_TO_SCALE = ['atomic_num', 'ave_bond_length', 'ave_neighbor_weight']\n",
        "BOND_FEATS_TO_SCALE = ['dist']\n",
        "\n",
        "RAW_DATA_PATH = './champs-scalar-coupling/'\n",
        "PATH = './champs-scalar-coupling/tmp/'\n",
        "PROC_DATA_PATH = './champs-scalar-coupling/proc_data/'\n",
        "SUB_PATH = './champs-scalar-coupling/submissions/'\n",
        "OOF_PATH = './champs-scalar-coupling/oofs/'\n",
        "\n",
        "\n",
        "def set_seed(seed=100):\n",
        "    \"\"\"Set the seed for all relevant RNGs.\"\"\"\n",
        "    # python RNG\n",
        "    random.seed(seed)\n",
        "\n",
        "    # pytorch RNGs\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # numpy RNG\n",
        "    np.random.seed(seed)\n",
        "\n",
        "\n",
        "def print_progress(i, n, print_iter=10000):\n",
        "    if (i%print_iter)==0:\n",
        "        print(f'{strftime(\"%H:%M:%S\", localtime())} - {(100 * i / n):.2f} %')\n",
        "\n",
        "\n",
        "def store_submit(predictions, name, print_head=False):\n",
        "    if not isinstance(predictions, pd.DataFrame):\n",
        "        submit = pd.read_csv(C.RAW_DATA_PATH + 'sample_submission.csv')\n",
        "        submit['scalar_coupling_constant'] = predictions\n",
        "    else:\n",
        "        submit = predictions\n",
        "    submit.to_csv(f'{C.SUB_PATH}{name}-submission.csv', index=False)\n",
        "    if print_head: print(submit.head())\n",
        "\n",
        "\n",
        "def store_oof(predictions, name, print_head=False):\n",
        "    if not isinstance(predictions, pd.DataFrame):\n",
        "        oof = pd.DataFrame(predictions, columns=['scalar_coupling_constants'])\n",
        "    else:\n",
        "        oof = predictions\n",
        "    oof.to_csv(f'{C.OOF_PATH}{name}-oof.csv')\n",
        "    if print_head: print(oof.head())\n",
        "\n",
        "\n",
        "def scale_features(df, features, train_mol_ids=None, means=None, stds=None,\n",
        "                   return_mean_and_std=False):\n",
        "    if ((df[features].mean().abs()>0.1).any()\n",
        "        or ((df[features].std()-1.0).abs()>0.1).any()):\n",
        "        if train_mol_ids is not None:\n",
        "            idx = df['molecule_id'].isin(train_mol_ids)\n",
        "            means = df.loc[idx, features].mean()\n",
        "            stds = df.loc[idx, features].std()\n",
        "        else:\n",
        "            assert means is not None\n",
        "            assert stds is not None\n",
        "        df[features] = (df[features] - means) / stds\n",
        "    if return_mean_and_std: return df, means, stds\n",
        "    else: return df"
      ],
      "metadata": {
        "id": "S6RzZkW02Q7k"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from fastai.basic_train import Learner, LearnerCallback, Callback, add_metrics\n",
        "from fastai.callback import annealing_cos\n",
        "from fastai.callbacks import SaveModelCallback\n",
        "from fastai.callbacks.general_sched import TrainingPhase, GeneralScheduler\n",
        "\n",
        "\n",
        "def reshape_targs(targs, mask_val= BATCH_PAD_VAL):   # C.BATCH_PAD_VAL = -999\n",
        "    targs = targs.view(-1, targs.size(-1))\n",
        "    return targs[targs[:,0]!=mask_val]\n",
        "\n",
        "def group_mean_log_mae(y_true, y_pred, types, sc_mean=0, sc_std=1):\n",
        "    def proc(x): \n",
        "        if isinstance(x, torch.Tensor): return x.cpu().numpy().ravel() \n",
        "    y_true, y_pred, types = proc(y_true), proc(y_pred), proc(types)\n",
        "    y_true = sc_mean + y_true * sc_std\n",
        "    y_pred = sc_mean + y_pred * sc_std\n",
        "    maes = pd.Series(y_true - y_pred).abs().groupby(types).mean()\n",
        "    gmlmae = np.log(maes).mean()\n",
        "    return gmlmae\n",
        "        \n",
        "def contribs_rmse_loss(preds, targs):\n",
        "    \"\"\"\n",
        "    Returns the sum of RMSEs for each scalar coupling (sc) contribution and \n",
        "    the sc constant in a batch.\n",
        "    \n",
        "    Args:\n",
        "        - preds: tensor of shape (n_sc_batch, 5) containing predictions. Last \n",
        "        column is the scalar coupling constant.\n",
        "        - targs: tensor of shape (batch_size, max_n_sc_per_molecule, 5) \n",
        "        containing true values. Last column is the scalar coupling constant.\n",
        "    \"\"\"\n",
        "    targs = reshape_targs(targs)\n",
        "    return torch.mean((preds - targs) ** 2, dim=0).sqrt().sum()\n",
        "\n",
        "def rmse(preds, targs):\n",
        "    targs = reshape_targs(targs)\n",
        "    return torch.sqrt(F.mse_loss(preds[:,-1], targs[:,-1]))\n",
        "\n",
        "def mae(preds, targs):\n",
        "    targs = reshape_targs(targs)\n",
        "    return torch.abs(preds[:,-1] - targs[:,-1]).mean()\n",
        "\n",
        "import pdb\n",
        "import numpy as np\n",
        "from visdom import Visdom\n",
        "\n",
        "class VisdomLinePlotter(object):\n",
        "    \"\"\"Plots to Visdom\"\"\"\n",
        "    def __init__(self, env_name='main'):\n",
        "        self.viz = Visdom(port=5919)\n",
        "        self.env = env_name\n",
        "        self.plots = {}\n",
        "    def plot(self, var_name, split_name, x, y, env=None, x_label='Epochs'):\n",
        "        if env is not None:\n",
        "            print_env = env\n",
        "        else:\n",
        "            print_env = self.env\n",
        "        if var_name not in self.plots:\n",
        "            self.plots[var_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=print_env, opts=dict(\n",
        "                legend=[split_name],\n",
        "                title=var_name,\n",
        "                xlabel=x_label,\n",
        "                ylabel=var_name))\n",
        "        else:\n",
        "            self.viz.line(X=np.array([x]), Y=np.array([y]), env=print_env, win=self.plots[var_name], name=split_name, update='append')\n",
        "\n",
        "plotter = VisdomLinePlotter(env_name='fangjia')\n",
        "\n",
        "class GradientClipping(LearnerCallback):\n",
        "    \"Gradient clipping during training after 'start_it' number of steps.\"\n",
        "    def __init__(self, learn:Learner, clip:float = 0., start_it:int = 100):\n",
        "        super().__init__(learn)\n",
        "        self.clip, self.start_it = clip, start_it\n",
        "\n",
        "    def on_backward_end(self, iteration, **kwargs):\n",
        "        \"Clip the gradient before the optimizer step.\"\n",
        "        if self.clip and (iteration > self.start_it):\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                self.learn.model.parameters(), self.clip)\n",
        "\n",
        "class GroupMeanLogMAE(Callback):\n",
        "    \"\"\"Callback to report the group mean log MAE during training.\"\"\"\n",
        "    _order = -20 # Needs to run before the recorder\n",
        "    metrics_list_train, metrics_list_valid = [], []\n",
        "    sc_types_valid, output_valid, target_valid = [], [], []\n",
        "    def __init__(self, learn, **kwargs):\n",
        "        self.learn = learn\n",
        "\n",
        "    def on_train_begin(self, **kwargs):\n",
        "        metric_names = ['group_mean_log_mae']\n",
        "        self.learn.recorder.add_metric_names(metric_names)\n",
        "\n",
        "    def on_epoch_begin(self, **kwargs):\n",
        "        self.sc_types_train, self.output_train, self.target_train = [], [], []\n",
        "        self.sc_types_valid, self.output_valid, self.target_valid = [], [], []\n",
        "    \n",
        "#    def on_batch_begin(self, **kwargs):\n",
        "#        self.sc_types_valid, self.output_valid, self.target_valid = [], [], []\n",
        "\n",
        "    def on_batch_end(self, last_target, last_output, last_input, train, **kwargs):\n",
        "        sc_types = last_input[-1].view(-1)\n",
        "        mask = sc_types != C.BATCH_PAD_VAL\n",
        "        if train:\n",
        "            self.sc_types_train.append(sc_types[mask])\n",
        "            self.output_train.append(last_output[:,-1])\n",
        "            self.target_train.append(reshape_targs(last_target)[:,-1])\n",
        "        else:\n",
        "            #pdb.set_trace()\n",
        "            self.sc_types_valid.append(sc_types[mask])\n",
        "            self.output_valid.append(last_output[:,-1])\n",
        "            self.target_valid.append(reshape_targs(last_target)[:,-1])\n",
        "#            torch.save(self.sc_types_valid[-1], '/home/nesa/fangjia/kaggle-champs-master--0/sc_types_valid.pt')\n",
        "#            torch.save(self.output_valid[-1], '/home/nesa/fangjia/kaggle-champs-master--0/output_valid.pt')\n",
        "#            torch.save(self.target_valid[-1], '/home/nesa/fangjia/kaggle-champs-master--0/target_valid.pt')\n",
        "\n",
        "    def on_epoch_end(self, epoch, last_metrics, **kwargs):\n",
        "        if (len(self.sc_types_train) > 0) and (len(self.output_train) > 0):\n",
        "            sc_types_train = torch.cat(self.sc_types_train)\n",
        "            preds_train = torch.cat(self.output_train)\n",
        "            target_train = torch.cat(self.target_train)\n",
        "            metrics_train = [group_mean_log_mae(preds_train, target_train, sc_types_train, C.SC_MEAN, C.SC_STD)] \n",
        "            plotter.plot('MPNN_Transformer', 'train', epoch, metrics_train[0])\n",
        "            self.metrics_list_train.append(metrics_train[0])\n",
        "            #torch.save(self.metrics_list_train,'/home/nesa/fangjia/kaggle-champs-master--0/metrics_feat_attn_train.pt')           \n",
        "            #torch.save(self.metrics_list_train,'/home/nesa/fangjia/kaggle-champs-master--0/metrics_train.pt')\n",
        "            #torch.save(self.metrics_list_train,'/home/nesa/fangjia/kaggle-champs-master--0/metrics_attn_train.pt')\n",
        "            torch.save(self.metrics_list_train,'/home/nesa/fangjia/kaggle-champs-master--0/metrics_feat_train.pt')\n",
        "            \n",
        "            sc_types_valid = torch.cat(self.sc_types_valid)\n",
        "            print(sc_types_valid.shape)\n",
        "            preds_valid = torch.cat(self.output_valid)\n",
        "            target_valid = torch.cat(self.target_valid)     \n",
        "            metrics_valid = [group_mean_log_mae(preds_valid, target_valid, sc_types_valid, C.SC_MEAN, C.SC_STD)]\n",
        "            plotter.plot('MPNN_Transformer', 'valid', epoch, metrics_valid[0])\n",
        "            self.metrics_list_valid.append(metrics_valid[0])\n",
        "            #torch.save(self.metrics_list_valid,'/home/nesa/fangjia/kaggle-champs-master--0/metrics_feat_attn_valid.pt')\n",
        "            #torch.save(self.metrics_list_valid,'/home/nesa/fangjia/kaggle-champs-master--0/metrics_valid.pt')\n",
        "            #torch.save(self.metrics_list_valid,'/home/nesa/fangjia/kaggle-champs-master--0/metrics_attn_valid.pt')\n",
        "            torch.save(self.metrics_list_valid,'/home/nesa/fangjia/kaggle-champs-master--0/metrics_feat_valid.pt')\n",
        "            #return add_metrics(last_metrics, metrics_train)\n",
        "            return add_metrics(last_metrics, metrics_valid)\n",
        "\n",
        "# Fastai's automatic loading was causing CUDA memory errors during snapshot\n",
        "# ensembling. The function below is a workaround.\n",
        "def save_model_cb_jump_to_epoch_adj(cb, epoch:int)->None:\n",
        "    \"\"\"Overwrites standard jump_to_epoch for the SaveModelCallback.\"\"\"\n",
        "    print(f'Model {cb.name}_{epoch-1} not loaded.')\n",
        "#SaveModelCallback.jump_to_epoch = save_model_cb_jump_to_epoch_adj"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "cP9FKTpU2Xer",
        "outputId": "ead47622-81f9-41f1-9686-73f58a6fd5ed"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-ec3ebf593dcc>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvisdom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisdom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mVisdomLinePlotter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'visdom'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_existing_group(gb, i):\n",
        "    group_df = gb.get_group(i)\n",
        "    return group_df\n",
        "\n",
        "\n",
        "def get_dist_matrix(struct_df):\n",
        "    locs = struct_df[['x', 'y', 'z']].values\n",
        "    n_atoms = len(locs)\n",
        "    loc_tile = np.tile(locs.T, (n_atoms, 1, 1))\n",
        "    dist_mat = np.sqrt(((loc_tile - loc_tile.T) ** 2).sum(axis=1))\n",
        "    return dist_mat\n",
        "\n",
        "\n",
        "class MoleculeDataset(Dataset):\n",
        "    \"\"\"Dataset returning inputs and targets per molecule.\"\"\"\n",
        "\n",
        "    def __init__(self, mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_bond,\n",
        "                 gb_mol_struct, gb_mol_angle_in, gb_mol_angle_out,\n",
        "                 gb_mol_graph_dist):\n",
        "        \"\"\"Dataset is constructed from dataframes grouped by molecule_id.\"\"\"\n",
        "        self.n = len(mol_ids)\n",
        "        self.mol_ids = mol_ids\n",
        "        self.gb_mol_sc = gb_mol_sc\n",
        "        self.gb_mol_atom = gb_mol_atom\n",
        "        self.gb_mol_bond = gb_mol_bond\n",
        "        self.gb_mol_struct = gb_mol_struct\n",
        "        self.gb_mol_angle_in = gb_mol_angle_in\n",
        "        self.gb_mol_angle_out = gb_mol_angle_out\n",
        "        self.gb_mol_graph_dist = gb_mol_graph_dist\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.gb_mol_sc.get_group(self.mol_ids[idx]),\n",
        "                self.gb_mol_atom.get_group(self.mol_ids[idx]),\n",
        "                self.gb_mol_bond.get_group(self.mol_ids[idx]),\n",
        "                self.gb_mol_struct.get_group(self.mol_ids[idx]),\n",
        "                self.gb_mol_angle_in.get_group(self.mol_ids[idx]),\n",
        "                _get_existing_group(self.gb_mol_angle_out, self.mol_ids[idx]),\n",
        "                self.gb_mol_graph_dist.get_group(self.mol_ids[idx]))\n",
        "\n",
        "\n",
        "def arr_lst_to_padded_batch(arr_lst, dtype=torch.float,\n",
        "                            pad_val=C.BATCH_PAD_VAL):\n",
        "    tensor_list = [torch.Tensor(arr).type(dtype) for arr in arr_lst]\n",
        "    batch = torch.nn.utils.rnn.pad_sequence(\n",
        "        tensor_list, batch_first=True, padding_value=pad_val)\n",
        "    return batch.contiguous()\n",
        "\n",
        "\n",
        "def collate_parallel_fn(batch, test=False):\n",
        "    \"\"\"\n",
        "    Transforms input dataframes grouped by molecule into a batch of input and \n",
        "    target tensors for a 'batch_size' number of molecules. The first dimension \n",
        "    is used as the batch dimension.\n",
        "\n",
        "    Returns:\n",
        "        - atom_x: features at the atom level\n",
        "        - bond_x: features at the chemical bond level\n",
        "        - sc_x: features describing the scalar coupling atom_0 and atom_1 pairs\n",
        "        - sc_m_x: in addition to the set of features in 'sc_x', includes \n",
        "            features at the molecule level.\n",
        "        - eucl_dists: 3D euclidean distance matrices\n",
        "        - graph_dists: graph distance matrices\n",
        "        - angles: cosine angles between all chemical bonds\n",
        "        - mask: binary mask of dim=(batch_size, max_n_atoms, max_n_atoms),\n",
        "            where max_n_atoms is the largest number of atoms per molecule in \n",
        "            'batch'\n",
        "        - bond_idx: tensor of dim=(batch_size, max_n_bonds, 2), containing the\n",
        "            indices of atom_0 and atom_1 pairs that form chemical bonds\n",
        "        - sc_idx: tensor of dim=(batch_size, max_n_sc, 2), containing the\n",
        "            indices of atom_0 and atom_1 pairs that form a scalar coupling\n",
        "            pair\n",
        "        - angles_idx: tensor of dim=(batch_size, max_n_angles, 1), mapping \n",
        "            angles to the chemical bonds in the molecule.\n",
        "        - sc_types: scalar coupling types\n",
        "        - sc_vals: scalar coupling contributions (first 4 columns) and constant\n",
        "            (last column)\n",
        "    \"\"\"\n",
        "    batch_size, n_atom_sum, n_pairs_sum = len(batch), 0, 0\n",
        "    atom_x, bond_x, sc_x, sc_m_x = [], [], [], []\n",
        "    eucl_dists, graph_dists = [], []\n",
        "    angles_in, angles_out = [], []\n",
        "    mask, bond_idx, sc_idx = [], [], []\n",
        "    angles_in_idx, angles_out_idx = [], []\n",
        "    sc_types, sc_vals = [], []\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        (sc_df, atom_df, bond_df, struct_df, angle_in_df, angle_out_df,\n",
        "         graph_dist_df) = batch[b]\n",
        "        n_atoms, n_pairs, n_sc = len(atom_df), len(bond_df), len(sc_df)\n",
        "        n_pad = C.MAX_N_ATOMS - n_atoms\n",
        "        eucl_dists_ = get_dist_matrix(struct_df)\n",
        "        eucl_dists_ = np.pad(eucl_dists_, [(0, 0), (0, n_pad)], 'constant',\n",
        "                             constant_values=999)\n",
        "\n",
        "        atom_x.append(atom_df[C.ATOM_FEATS].values)\n",
        "        bond_x.append(bond_df[C.BOND_FEATS].values)\n",
        "        sc_x.append(sc_df[C.SC_EDGE_FEATS].values)\n",
        "        sc_m_x.append(sc_df[C.SC_MOL_FEATS].values)\n",
        "        sc_types.append(sc_df['type'].values)\n",
        "        if not test:\n",
        "            n_sc_pad = C.MAX_N_SC - n_sc\n",
        "            sc_vals_ = sc_df[C.CONTRIB_COLS + [C.TARGET_COL]].values\n",
        "            sc_vals.append(np.pad(sc_vals_, [(0, n_sc_pad), (0, 0)], 'constant',\n",
        "                                  constant_values=-999))\n",
        "        eucl_dists.append(eucl_dists_)\n",
        "        graph_dists.append(graph_dist_df.values[:, :-1])\n",
        "        angles_in.append(angle_in_df['cos_angle'].values)\n",
        "        if angle_out_df is not None:\n",
        "            angles_out.append(angle_out_df['cos_angle'].values)\n",
        "        else:\n",
        "            angles_out.append(np.array([C.BATCH_PAD_VAL]))\n",
        "\n",
        "        mask.append(np.pad(np.ones(2 * [n_atoms]), [(0, 0), (0, n_pad)],\n",
        "                           'constant'))\n",
        "        bond_idx.append(bond_df[['idx_0', 'idx_1']].values)\n",
        "        sc_idx.append(sc_df[['atom_index_0', 'atom_index_1']].values)\n",
        "        angles_in_idx.append(angle_in_df['b_idx'].values)\n",
        "        if angle_out_df is not None:\n",
        "            angles_out_idx.append(angle_out_df['b_idx'].values)\n",
        "        else:\n",
        "            angles_out_idx.append(np.array([0.]))\n",
        "\n",
        "        n_atom_sum += n_atoms\n",
        "        n_pairs_sum += n_pairs\n",
        "\n",
        "    atom_x = arr_lst_to_padded_batch(atom_x, pad_val=0.)\n",
        "    bond_x = arr_lst_to_padded_batch(bond_x)\n",
        "    max_n_atoms = atom_x.size(1)\n",
        "    max_n_bonds = bond_x.size(1)\n",
        "    angles_out_idx = [a + max_n_bonds for a in angles_out_idx]\n",
        "\n",
        "    sc_x = arr_lst_to_padded_batch(sc_x)\n",
        "    sc_m_x = arr_lst_to_padded_batch(sc_m_x)\n",
        "    if not test:\n",
        "        sc_vals = arr_lst_to_padded_batch(sc_vals)\n",
        "    else:\n",
        "        sc_vals = torch.tensor([0.] * batch_size)\n",
        "    sc_types = arr_lst_to_padded_batch(sc_types, torch.long)\n",
        "    mask = arr_lst_to_padded_batch(mask, torch.uint8, 0)\n",
        "    mask = mask[:, :, :max_n_atoms].contiguous()\n",
        "    bond_idx = arr_lst_to_padded_batch(bond_idx, torch.long, 0)\n",
        "    sc_idx = arr_lst_to_padded_batch(sc_idx, torch.long, 0)\n",
        "    angles_in_idx = arr_lst_to_padded_batch(angles_in_idx, torch.long, 0)\n",
        "    angles_out_idx = arr_lst_to_padded_batch(angles_out_idx, torch.long, 0)\n",
        "    angles_idx = torch.cat((angles_in_idx, angles_out_idx), dim=-1).contiguous()\n",
        "    eucl_dists = arr_lst_to_padded_batch(eucl_dists, pad_val=999)\n",
        "    eucl_dists = eucl_dists[:, :, :max_n_atoms].contiguous()\n",
        "    graph_dists = arr_lst_to_padded_batch(graph_dists, torch.long, 10)\n",
        "    graph_dists = graph_dists[:, :, :max_n_atoms].contiguous()\n",
        "    angles_in = arr_lst_to_padded_batch(angles_in)\n",
        "    angles_out = arr_lst_to_padded_batch(angles_out)\n",
        "    angles = torch.cat((angles_in, angles_out), dim=-1).contiguous()\n",
        "\n",
        "    return (atom_x, bond_x, sc_x, sc_m_x, eucl_dists, graph_dists, angles, mask,\n",
        "            bond_idx, sc_idx, angles_idx, sc_types), sc_vals\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "    \"\"\"Produce N identical layers.\"\"\"\n",
        "    return torch.nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, dropout):\n",
        "        super().__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"\"\"Apply residual connection to any sublayer with the same size.\"\"\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "\n",
        "def _gather_nodes(x, idx, sz_last_dim):                   # x: 1*14*64, idx: 1*28, sz_last_dim: 64\n",
        "    idx = idx.unsqueeze(-1).expand(-1, -1, sz_last_dim)   # idx: 1*28*64\n",
        "    return x.gather(1, idx)                               # return: 1*28*64\n",
        "\n",
        "\n",
        "class ENNMessage(nn.Module):\n",
        "    \"\"\"\n",
        "    The edge network message passing function from the MPNN paper.Optionally\n",
        "    adds and additional cosine angle based attention mechanism over incoming\n",
        "    messages.\n",
        "    \"\"\"\n",
        "    PAD_VAL = -999\n",
        "\n",
        "    def __init__(self, d_model, d_edge, kernel_sz, enn_args={}, ann_args=None):\n",
        "        super().__init__()\n",
        "        assert kernel_sz <= d_model\n",
        "        self.d_model, self.kernel_sz = d_model, kernel_sz\n",
        "        self.enn = FullyConnectedNet(d_edge, d_model * kernel_sz, **enn_args)\n",
        "        if ann_args:\n",
        "            self.ann = FullyConnectedNet(1, d_model, **ann_args)\n",
        "        else:\n",
        "            self.ann = None\n",
        "\n",
        "    def forward(self, x, edges, pairs_idx, angles=None, angles_idx=None, t=0):\n",
        "        \"\"\"Note that edges and pairs_idx raw inputs are for a unidirectional\n",
        "        graph. They are expanded to allow bidirectional message passing.\"\"\"\n",
        "        if t == 0:\n",
        "            self.set_a_mat(edges)                       # edges: 1*14*8\n",
        "            if self.ann: self.set_attn(angles)          # angles: 1*44\n",
        "            # concat reversed pairs_idx for bidirectional message passing\n",
        "            self.pairs_idx = torch.cat([pairs_idx, pairs_idx[:, :, [1, 0]]], dim=1) # pairs_idx: 1*14*2 --> self.pairs_idx: 1*28*2\n",
        "        return self.add_message(torch.zeros_like(x), x, angles_idx)         # torch.zeros_like(x): 1*14*64\n",
        "\n",
        "    def set_a_mat(self, edges):          # edges: 1*14*8  d_edge: 8\n",
        "        n_edges = edges.size(1)          # n_edges: 14\n",
        "        a_vect = self.enn(edges)         # d_model: 64;  kernel_sz: 64  a_vect: 1*14*4096\n",
        "        a_vect = a_vect / (self.kernel_sz ** .5)  # rescale\n",
        "        mask = edges[:, :, 0, None].expand(a_vect.size()) == self.PAD_VAL   # mask: 1*14*4096\n",
        "        a_vect = a_vect.masked_fill(mask, 0.0)   # a_vect: 1*14*4096\n",
        "        self.a_mat = a_vect.view(-1, n_edges, self.d_model, self.kernel_sz)    # a_mask: 1*14*64*64\n",
        "        # concat a_mats for bidirectional message passing\n",
        "        self.a_mat = torch.cat([self.a_mat, self.a_mat], dim=1)       # a_mask: 1*28*64*64\n",
        "\n",
        "    def set_attn(self, angles):                                  # angles: 1*44\n",
        "        angles = angles.unsqueeze(-1)                            # angles: 1*44*1\n",
        "        self.attn = self.ann(angles)                             # self.attn: 1*44*64, d_model: 64\n",
        "        mask = angles.expand(self.attn.size()) == self.PAD_VAL   # mask: 1*44*64\n",
        "        self.attn = self.attn.masked_fill(mask, 0.0)             # self.attn: 1*44*64\n",
        "\n",
        "    def add_message(self, m, x, angles_idx=None):\n",
        "        \"\"\"Add message for atom_{i}: m_{i} += sum_{j}[attn_{ij} A_{ij}x_{j}].\"\"\"\n",
        "        # select the 'x_{j}' feeding into the 'm_{i}'\n",
        "        x_in = _gather_nodes(x, self.pairs_idx[:, :, 1], self.d_model)       # self.pairs_idx: 1*28*2 ; x_in: 1*28*64\n",
        "                                                                             # self.d_model: 64\n",
        "        # do the matrix multiplication 'A_{ij}x_{j}'\n",
        "        if self.kernel_sz == self.d_model:  # full matrix multiplcation      # self.kernel_sz: 64\n",
        "            ax = (x_in.unsqueeze(-2) @ self.a_mat).squeeze(-2)               # self.a_mat: 1*28*64*64  ax: 1*28*64\n",
        "        else:  # do a convolution\n",
        "            x_padded = F.pad(x_in, self.n_pad)\n",
        "            x_unfolded = x_padded.unfold(-1, self.kernel_sz, 1)\n",
        "            ax = (x_unfolded * self.a_mat).sum(-1)\n",
        "\n",
        "        # apply atttention\n",
        "        if self.ann:\n",
        "            n_pairs = self.pairs_idx.size(1)                            # n_pairs: 28\n",
        "            # average all attn(angle_{ijk}) per edge_{ij}.\n",
        "            # i.e.: attn_{ij} = sum_{k}[attn(angle_{ijk})] / n_angles_{ij}\n",
        "            ave_att = scatter_mean(self.attn, angles_idx, num=n_pairs, dim=1,\n",
        "                                   out=torch.ones_like(ax))            # angles_idx: 44;   self.attn: 1*44*64\n",
        "            ax = ave_att * ax                                          # ax: 1*28*64       ax, ave_att: 1*28*64\n",
        "\n",
        "        # sum up all 'A_{ij}h_{j}' per node 'i'\n",
        "        idx_0 = self.pairs_idx[:, :, 0, None].expand(-1, -1, self.d_model)   # idx_0: 1*28*64\n",
        "        return m.scatter_add(1, idx_0, ax)                  # m=torch.zeros_like(x): 1*14*64;\n",
        "                                                            # return: 1*14*64\n",
        "    @property\n",
        "    def n_pad(self):\n",
        "        k = self.kernel_sz\n",
        "        return (k // 2, k // 2 - int(k % 2 == 0))\n",
        "\n",
        "\n",
        "class MultiHeadedDistAttention(nn.Module):\n",
        "    \"\"\"Generalizes the euclidean and graph distance based attention layers.\"\"\"\n",
        "\n",
        "    def __init__(self, h, d_model):   # h: 1, d_model: 64\n",
        "        super().__init__()\n",
        "        self.d_model, self.d_k, self.h = d_model, d_model // h, h    # self.d_model: 64, self.d_k: 64, self.h: 1\n",
        "        self.attn = None\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 2)\n",
        "\n",
        "    def forward(self, dists, x, mask):                     # dists: 1*14*14,  x: 1*14*64,  mask: 1*14*14\n",
        "        batch_size = x.size(0)                             # batch_size: 1\n",
        "        x = self.linears[0](x).view(batch_size, -1, self.h, self.d_k)   # x: 1*14*64 --> 1*14*1*64\n",
        "        x, self.attn = self.apply_attn(dists, x, mask)                  # x: 1*14*1*64,  self.attn: 1*1*14*14\n",
        "        x = x.view(batch_size, -1, self.h * self.d_k)                   # x: 1*14*64\n",
        "        return self.linears[-1](x)                                      # x: 1*14*64\n",
        "\n",
        "    def apply_attn(self, dists, x, mask):                    # dists: 1*14*14, x: 1*14*1*64, mask: 1*14*14\n",
        "        attn = self.create_raw_attn(dists, mask)             # attn: 1*14*14*1\n",
        "        attn = attn.transpose(-2, -1).transpose(1, 2)        # attn: 1*1*14*14\n",
        "        x = x.transpose(1, 2)                                # x: 1*1*14*64\n",
        "        x = torch.matmul(attn, x)                            # x: 1*1*14*64\n",
        "        x = x.transpose(1, 2).contiguous()                   # x: 1*14*1*64\n",
        "        return x, attn                                       # x: 1*14*1*64,  attn: 1*1*14*14\n",
        "\n",
        "    def create_raw_attn(self, dists, mask):\n",
        "        pass\n",
        "\n",
        "\n",
        "class MultiHeadedGraphDistAttention(MultiHeadedDistAttention):\n",
        "    \"\"\"Attention based on an embedding of the graph distance matrix.\"\"\"\n",
        "    MAX_GRAPH_DIST = 10\n",
        "\n",
        "    def __init__(self, h, d_model):\n",
        "        super().__init__(h, d_model)                                # h: 1,  d_model: 64\n",
        "        self.embedding = nn.Embedding(self.MAX_GRAPH_DIST + 1, h)   #\n",
        "\n",
        "    def create_raw_attn(self, dists, mask):                         # dists: 1*14*14,  mask: 1*14*14\n",
        "        emb_dists = self.embedding(dists)                           # emb_dists: 1*14*14*1\n",
        "        mask = mask.unsqueeze(-1).expand(emb_dists.size())          # mask: 1*14*14*1\n",
        "        emb_dists = emb_dists.masked_fill(mask == 0, -1e9)          # emb_dists: 1*14*14*1\n",
        "        return F.softmax(emb_dists, dim=-2).masked_fill(mask == 0, 0)   # return: 1*14*14*1\n",
        "\n",
        "\n",
        "class MultiHeadedEuclDistAttention(MultiHeadedDistAttention):\n",
        "    \"\"\"Attention based on a parameterized normal pdf taking a molecule's\n",
        "    euclidean distance matrix as input.\"\"\"\n",
        "\n",
        "    def __init__(self, h, d_model):                              # h: 1,  d_model: 64\n",
        "        super().__init__(h, d_model)\n",
        "        self.log_prec = nn.Parameter(torch.Tensor(1, 1, 1, h))   # self.log_prec: 1*1*1*1\n",
        "        self.locs = nn.Parameter(torch.Tensor(1, 1, 1, h))       # self.locs: 1*1*1*1\n",
        "        nn.init.normal_(self.log_prec, mean=0.0, std=0.1)        # 初始归一化\n",
        "        nn.init.normal_(self.locs, mean=0.0, std=1.0)\n",
        "\n",
        "    def create_raw_attn(self, dists, mask):                      # dists: 1*14*14,  mask: 1*14*14\n",
        "        dists = dists.unsqueeze(-1).expand(-1, -1, -1, self.h)   # dists: 1*14*14*1\n",
        "        z = torch.exp(self.log_prec) * (dists - self.locs)       # z: 1*14*14*1\n",
        "        pdf = torch.exp(-0.5 * z ** 2)                           # pdf: 1*14*14*1\n",
        "        return pdf / pdf.sum(dim=-2, keepdim=True).clamp(1e-9)   # return: 1*14*14*1/1*14*1*1\n",
        "\n",
        "\n",
        "def attention(query, key, value, mask=None, dropout=None):  # query: 1*1*14*64, key: 1*1*14*64, value: 1*1*14*64\n",
        "    \"\"\"Compute 'Scaled Dot Product Attention'.\"\"\"           # mask: 1*1*14*14\n",
        "    d_k = query.size(-1)           # d_k: 64\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)  # scores: 1*1*14*14\n",
        "    if mask is not None: scores = scores.masked_fill(mask == 0, -1e9)     #\n",
        "    p_attn = F.softmax(scores, dim=-1).masked_fill(mask == 0, 0)          # p_attn: 1*1*14*14\n",
        "    if dropout is not None: p_attn = dropout(p_attn)                      #\n",
        "    return torch.matmul(p_attn, value), p_attn                            # return: 1*1*14*64, p_attn: 1*1*14*14\n",
        "\n",
        "\n",
        "class MultiHeadedSelfAttention(nn.Module):\n",
        "    \"\"\"Applies self-attention as described in the Transformer paper.\"\"\"\n",
        "\n",
        "    def __init__(self, h, d_model, dropout=0.1):                          # d_model: 64,   h: 1\n",
        "        super().__init__()\n",
        "        self.d_model, self.d_k, self.h = d_model, d_model // h, h         # self.d_model: 64, self.d_k: 64, self.h: 1\n",
        "        self.attn = None\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.dropout = nn.Dropout(p=dropout) if dropout > 0.0 else None\n",
        "\n",
        "    def forward(self, x, mask):                                 # x: 1*14*64,  mask: 1*14*14\n",
        "        # Same mask applied to all h heads.\n",
        "        mask = mask.unsqueeze(1)                                # mask: 1*1*14*14\n",
        "        batch_size = x.size(0)                                  # batch_size: 1\n",
        "\n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
        "        query, key, value = [\n",
        "            l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "            for l in self.linears[:3]\n",
        "        ]                                                       # query, key, value: 1*1*14*64\n",
        "\n",
        "        # 2) Apply attention on all the projected vectors in batch.\n",
        "        x, self.attn = attention(query, key, value, mask, self.dropout)   # x: 1*1*14*64, self.attn: 1*1*14*14\n",
        "\n",
        "        # 3) \"Concat\" using a view and apply a final linear.\n",
        "        x = x.transpose(1, 2).contiguous()                                # x: 1*14*1*64\n",
        "        x = x.view(batch_size, -1, self.d_model)                          # x: 1*14*64\n",
        "        return self.linears[-1](x)                                        # x: 1*14*64\n",
        "\n",
        "\n",
        "class AttendingLayer(nn.Module):\n",
        "    \"\"\"Stacks the three attention layers and the pointwise feedforward net.\"\"\"\n",
        "\n",
        "    def __init__(self, size, eucl_dist_attn, graph_dist_attn, self_attn, ff,\n",
        "                 dropout):                      # size: 64\n",
        "        super().__init__()\n",
        "        self.eucl_dist_attn = eucl_dist_attn    # eucl_dist_attn:\n",
        "                                                # ModuleList((0): Linear(in_features=64, out_features=64, bias=True)\n",
        "                                                # (1): Linear(in_features=64, out_features=64, bias=True))\n",
        "\n",
        "        self.graph_dist_attn = graph_dist_attn  # graph_dist_attn:\n",
        "                                                # ModuleList((0): Linear(in_features=64, out_features=64, bias=True)\n",
        "                                                # (1): Linear(in_features=64, out_features=64, bias=True))\n",
        "                                                # (embedding): Embedding(11, 1))\n",
        "\n",
        "        self.self_attn = self_attn              # self_attn:\n",
        "                                                # ModuleList((0): Linear(in_features=64, out_features=64, bias=True)\n",
        "                                                # (1): Linear(in_features=64, out_features=64, bias=True)\n",
        "                                                # (2): Linear(in_features=64, out_features=64, bias=True)\n",
        "                                                # (3): Linear(in_features=64, out_features=64, bias=True))\n",
        "\n",
        "        self.ff = ff                            # ff:\n",
        "                                                # Sequential((0): Linear(in_features=64, out_features=256, bias=True)\n",
        "                                                # (1): ReLU(inplace=True)\n",
        "                                                # (2): Linear(in_features=256, out_features=64, bias=True))\n",
        "\n",
        "        self.subconns = clones(SublayerConnection(size, dropout), 4)\n",
        "                                                # self.subconns:\n",
        "                                                # ModuleList(\n",
        "                                                # (0): SublayerConnection(\n",
        "                                                # (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
        "                                                # (dropout): Dropout(p=0.0, inplace=False))\n",
        "\n",
        "                                                # (1): SublayerConnection(\n",
        "                                                # (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
        "                                                # (dropout): Dropout(p=0.0, inplace=False))\n",
        "\n",
        "                                                # (2): SublayerConnection(\n",
        "                                                # (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
        "                                                # (dropout): Dropout(p=0.0, inplace=False))\n",
        "\n",
        "                                                # (3): SublayerConnection(\n",
        "                                                # (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
        "                                                # (dropout): Dropout(p=0.0, inplace=False))\n",
        "\n",
        "        self.size = size                        # size: 64\n",
        "\n",
        "    def forward(self, x, eucl_dists, graph_dists, mask):   # x: 1*14*64, eucl_dists, graph_dists: 1*14*14, mask: 1*14*14\n",
        "        eucl_dist_sub = lambda x: self.eucl_dist_attn(eucl_dists, x, mask)    #\n",
        "        x = self.subconns[0](x, eucl_dist_sub)             # x: 1*14*64\n",
        "        graph_dist_sub = lambda x: self.graph_dist_attn(graph_dists, x, mask)\n",
        "        x = self.subconns[1](x, graph_dist_sub)            # x: 1*14*64\n",
        "        self_sub = lambda x: self.self_attn(x, mask)\n",
        "        x = self.subconns[2](x, self_sub)                  # x: 1*14*64\n",
        "        return self.subconns[3](x, self.ff)                # return: 1*14*64\n",
        "\n",
        "\n",
        "class MessagePassingLayer(nn.Module):\n",
        "    \"\"\"Stacks the bond and scalar coupling pair message passing layers.\"\"\"\n",
        "\n",
        "    def __init__(self, size, bond_mess, sc_mess, dropout, N):    # size: 64\n",
        "\n",
        "                                                                 # bond_mess:\n",
        "                                                                 # ENNMessage(\n",
        "                                                                 #   (enn): FullyConnectedNet(\n",
        "                                                                 #     (layers): Sequential(\n",
        "                                                                 #       (0): Linear(in_features=8, out_features=64, bias=True)\n",
        "                                                                 #       (1): ReLU(inplace=True)\n",
        "                                                                 #       (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                 #       (3): Linear(in_features=64, out_features=64, bias=True)\n",
        "                                                                 #       (4): ReLU(inplace=True)\n",
        "                                                                 #       (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                 #       (6): Linear(in_features=64, out_features=64, bias=True)\n",
        "                                                                 #       (7): ReLU(inplace=True)\n",
        "                                                                 #       (8): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                 #       (9): Linear(in_features=64, out_features=4096, bias=True)\n",
        "                                                                 #     )\n",
        "                                                                 #   )\n",
        "                                                                 #   (ann): FullyConnectedNet(\n",
        "                                                                 #     (layers): Sequential(\n",
        "                                                                 #       (0): Linear(in_features=1, out_features=64, bias=True)\n",
        "                                                                 #       (1): ReLU(inplace=True)\n",
        "                                                                 #       (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                 #       (3): Linear(in_features=64, out_features=64, bias=True)\n",
        "                                                                 #       (4): Tanh()\n",
        "                                                                 #     )\n",
        "                                                                 #   )\n",
        "                                                                 # )\n",
        "\n",
        "                                                                 # sc_mess:\n",
        "                                                                 # ENNMessage(\n",
        "                                                                 #   (enn): FullyConnectedNet(\n",
        "                                                                 #     (layers): Sequential(\n",
        "                                                                 #       (0): Linear(in_features=16, out_features=64, bias=True)\n",
        "                                                                 #       (1): ReLU(inplace=True)\n",
        "                                                                 #       (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                 #       (3): Linear(in_features=64, out_features=64, bias=True)\n",
        "                                                                 #       (4): ReLU(inplace=True)\n",
        "                                                                 #       (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                 #       (6): Linear(in_features=64, out_features=64, bias=True)\n",
        "                                                                 #       (7): ReLU(inplace=True)\n",
        "                                                                 #       (8): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                 #       (9): Linear(in_features=64, out_features=4096, bias=True)\n",
        "                                                                 #     )\n",
        "                                                                 #   )\n",
        "                                                                 # )\n",
        "\n",
        "                                                                 # dropout: 0.0\n",
        "\n",
        "        super().__init__()\n",
        "        self.bond_mess = bond_mess\n",
        "        self.sc_mess = sc_mess\n",
        "        self.linears = clones(nn.Linear(size, size), 2 * N)      # N = 1\n",
        "                                                                 # ModuleList(\n",
        "                                                                 #   (0): Linear(in_features=64, out_features=64, bias=True)\n",
        "                                                                 #   (1): Linear(in_features=64, out_features=64, bias=True))\n",
        "\n",
        "        self.subconns = clones(SublayerConnection(size, dropout), 2 * N)    # self.subconns:\n",
        "                                                                            # ModuleList(\n",
        "                                                                 #   (0): SublayerConnection(\n",
        "                                                                 #     (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                 #     (dropout): Dropout(p=0.0, inplace=False)\n",
        "                                                                 #   )\n",
        "                                                                 #   (1): SublayerConnection(\n",
        "                                                                 #     (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                 #     (dropout): Dropout(p=0.0, inplace=False)\n",
        "                                                                 #   )\n",
        "                                                                 # )\n",
        "\n",
        "    def forward(self, x, bond_x, sc_pair_x, angles, mask, bond_idx, sc_idx,\n",
        "                angles_idx, t=0):                       # x: 1*14*64, bond_x: 1*14*8, sc_pair_x: 1*29*16, angles: 1*44\n",
        "                                                        # mask: 1*14*14, bond_idx: 1*14*2, sc_idx: 1*29*2, angles_idx: 1*44\n",
        "        bond_sub = lambda x: self.linears[2 * t](\n",
        "            self.bond_mess(x, bond_x, bond_idx, angles, angles_idx, t))   # bond_sub:\n",
        "        x = self.subconns[2 * t](x, bond_sub)           # x: 1*14*64\n",
        "        sc_sub = lambda x: self.linears[(2 * t) + 1](\n",
        "            self.sc_mess(x, sc_pair_x, sc_idx, t=t))\n",
        "        return self.subconns[(2 * t) + 1](x, sc_sub)    # return: 1*14*64\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder stacks N attention layers and one message passing layer.\"\"\"\n",
        "\n",
        "    def __init__(self, mess_pass_layer, attn_layer, N):    #\n",
        "        super().__init__()\n",
        "        self.mess_pass_layer = mess_pass_layer\n",
        "        self.attn_layers = clones(attn_layer, N)\n",
        "        self.norm = LayerNorm(attn_layer.size)\n",
        "\n",
        "    def forward(self, x, bond_x, sc_pair_x, eucl_dists, graph_dists, angles,\n",
        "                mask, bond_idx, sc_idx, angles_idx):      # x: 1*14*64, bond_x: 1*14*8, sc_pair_x: 1*29*16, eucl_dists: 1*14*14\n",
        "                                                          # graph_dists: 1*14*14, angles: 1*44, mask: 1*14*14\n",
        "                                                          # bond_idx: 1*14*2, sc_idx: 1*29*2, angles_idx: 1*44\n",
        "        \"\"\"Pass the inputs (and mask) through each block in turn. Note that for\n",
        "        each block the same message passing layer is used.\"\"\"\n",
        "        for t, attn_layer in enumerate(self.attn_layers):              # t = 0\n",
        "            x = self.mess_pass_layer(x, bond_x, sc_pair_x, angles, mask,\n",
        "                                     bond_idx, sc_idx, angles_idx, t)  # x: 1*14*64\n",
        "            x = attn_layer(x, eucl_dists, graph_dists, mask)           # x: 1*14*64\n",
        "        return self.norm(x)                                            # x: 1*14*64\n",
        "\n",
        "\n",
        "# After N blocks of message passing and attending, the encoded atom states are\n",
        "# transferred to the head of the model: a customized feed-forward net for\n",
        "# predicting the scalar coupling (sc) constant.\n",
        "\n",
        "# First the relevant pairs of atom states for each sc constant in the batch\n",
        "# are selected, concatenated and stacked. Also concatenated to the encoded\n",
        "# states are a set of raw molecule and sc pair specific features. These states\n",
        "# are fed into a residual block comprised of a dense layer followed by a type\n",
        "# specific dense layer of dimension 'd_ff' (the same as the dimension used for\n",
        "# the pointwise feed-forward net).\n",
        "\n",
        "# The processed states are passed through to a relatively small feed-forward\n",
        "# net, which predicts each sc contribution seperately plus a residual.\n",
        "# Ultimately, the predictions of these contributions and the residual are summed\n",
        "# to predict the sc constant.\n",
        "\n",
        "def create_contrib_head(d_in, d_ff, act, dropout=0.0, layer_norm=True):  # d_in: 153, d_ff: 16, act: ReLU(inplace=True)\n",
        "    layers = hidden_layer(d_in, d_ff, False, dropout, layer_norm, act)   #\n",
        "    layers += hidden_layer(d_ff, 1, False, 0.0)  # output layer\n",
        "    return nn.Sequential(*layers)          # Sequential((0): Linear(in_features=153, out_features=16, bias=True)\n",
        "                                           # (1): ReLU(inplace=True)\n",
        "                                           # (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
        "                                           # (3): Linear(in_features=16, out_features=1, bias=True))\n",
        "\n",
        "class ContribsNet(nn.Module):\n",
        "    \"\"\"The feed-forward net used for the sc contribution and final sc constant\n",
        "    predictions.\"\"\"\n",
        "    N_CONTRIBS = 5\n",
        "    CONTIB_SCALES = [1, 250, 45, 35, 500]  # scales used to make the 5 predictions of similar magnitude\n",
        "\n",
        "    def __init__(self, d_in, d_ff, vec_in, act, dropout=0.0, layer_norm=True):\n",
        "        super().__init__()           # d_in: 153, d_ff: 16, vec_in: 256, act: ReLU(inplace=True)\n",
        "        contrib_head = create_contrib_head(d_in, d_ff, act, dropout, layer_norm)   # contrib_head:\n",
        "                                                                                   # Sequential(\n",
        "                                                                                   # (0): Linear(in_features=153, out_features=16, bias=True)\n",
        "                                                                                   # (1): ReLU(inplace=True)\n",
        "                                                                                   # (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                                   # (3): Linear(in_features=16, out_features=1, bias=True))\n",
        "        self.blocks = clones(contrib_head, self.N_CONTRIBS)     # ModuleList(\n",
        "                                                                # (0): Sequential(\n",
        "                                                                # (0): Linear(in_features=153, out_features=16, bias=True)\n",
        "                                                                # (1): ReLU(inplace=True)\n",
        "                                                                # (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                # (3): Linear(in_features=16, out_features=1, bias=True)\n",
        "                                                                # )\n",
        "                                                                # (1): Sequential(\n",
        "                                                                # (0): Linear(in_features=153, out_features=16, bias=True)\n",
        "                                                                # (1): ReLU(inplace=True)\n",
        "                                                                # (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                # (3): Linear(in_features=16, out_features=1, bias=True)\n",
        "                                                                # )\n",
        "                                                                # (2): Sequential(\n",
        "                                                                # (0): Linear(in_features=153, out_features=16, bias=True)\n",
        "                                                                # (1): ReLU(inplace=True)\n",
        "                                                                # (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                # (3): Linear(in_features=16, out_features=1, bias=True)\n",
        "                                                                # )\n",
        "                                                                # (3): Sequential(\n",
        "                                                                # (0): Linear(in_features=153, out_features=16, bias=True)\n",
        "                                                                # (1): ReLU(inplace=True)\n",
        "                                                                # (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                # (3): Linear(in_features=16, out_features=1, bias=True)\n",
        "                                                                # )\n",
        "                                                                # (4): Sequential(\n",
        "                                                                # (0): Linear(in_features=153, out_features=16, bias=True)\n",
        "                                                                # (1): ReLU(inplace=True)\n",
        "                                                                # (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
        "                                                                # (3): Linear(in_features=16, out_features=1, bias=True)\n",
        "                                                                # )\n",
        "                                                                # )\n",
        "\n",
        "    def forward(self, x):                                                          # x: 29*153\n",
        "        ys = torch.cat(\n",
        "            [b(x) / s for b, s in zip(self.blocks, self.CONTIB_SCALES)], dim=-1)   # self.CONTIB_SCALES: [1, 250, 45, 35, 500]\n",
        "                                                                                   # ys: 29*5\n",
        "        return torch.cat([ys[:, :-1], ys.sum(dim=-1, keepdim=True)], dim=-1)       # ys.sum(dim=-1, keepdim=True): 29*1\n",
        "                                                                                   # ys[:, :-1]: 29*4\n",
        "                                                                                   # return: 29*5\n",
        "\n",
        "class MyCustomHead(nn.Module):\n",
        "    \"\"\"Joins the sc type specific residual block with the sc contribution\n",
        "    feed-forward net.\"\"\"\n",
        "    PAD_VAL = -999\n",
        "    N_TYPES = 8\n",
        "\n",
        "    def __init__(self, d_input, d_ff, d_ff_contribs, pre_layers=[],\n",
        "                 post_layers=[], act=nn.ReLU(True), dropout=3 * [0.], norm=False): # d_input: 153, d_ff: 256, d_ff_contribs: 16\n",
        "        super().__init__()\n",
        "        fc_pre = hidden_layer(d_input, d_ff, False, dropout[0], norm, act)  # fc_pre:\n",
        "                                                                            # [Linear(in_features=153, out_features=256, bias=True),\n",
        "                                                                            # ReLU(inplace=True),\n",
        "                                                                            # LayerNorm((256,), eps=1e-05, elementwise_affine=True)]\n",
        "        self.preproc = nn.Sequential(*fc_pre)                               # Sequential(\n",
        "                                                                            # (0): Linear(in_features=153, out_features=256, bias=True)\n",
        "                                                                            # (1): ReLU(inplace=True)\n",
        "                                                                            # (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True))\n",
        "        fc_type = hidden_layer(d_ff, d_input, False, dropout[1], norm, act)     # fc_type:[Linear(in_features=256, out_features=153, bias=True),\n",
        "                                                                                # ReLU(inplace=True),\n",
        "                                                                                # LayerNorm((153,), eps=1e-05, elementwise_affine=True)]\n",
        "        self.types_net = clones(nn.Sequential(*fc_type), self.N_TYPES)      # self.N_TYPES: 8\n",
        "        self.contribs_net = ContribsNet(\n",
        "            d_input, d_ff_contribs, d_ff, act, dropout[2], layer_norm=norm)  # self.contribs_net:\n",
        "                                                                             # ContribsNet(\n",
        "        #   (blocks): ModuleList(\n",
        "        #     (0): Sequential(\n",
        "        #       (0): Linear(in_features=153, out_features=16, bias=True)\n",
        "        #       (1): ReLU(inplace=True)\n",
        "        #       (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
        "        #       (3): Linear(in_features=16, out_features=1, bias=True)\n",
        "        #     )\n",
        "        #     (1): Sequential(\n",
        "        #       (0): Linear(in_features=153, out_features=16, bias=True)\n",
        "        #       (1): ReLU(inplace=True)\n",
        "        #       (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
        "        #       (3): Linear(in_features=16, out_features=1, bias=True)\n",
        "        #     )\n",
        "        #     (2): Sequential(\n",
        "        #       (0): Linear(in_features=153, out_features=16, bias=True)\n",
        "        #       (1): ReLU(inplace=True)\n",
        "        #       (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
        "        #       (3): Linear(in_features=16, out_features=1, bias=True)\n",
        "        #     )\n",
        "        #     (3): Sequential(\n",
        "        #       (0): Linear(in_features=153, out_features=16, bias=True)\n",
        "        #       (1): ReLU(inplace=True)\n",
        "        #       (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
        "        #       (3): Linear(in_features=16, out_features=1, bias=True)\n",
        "        #     )\n",
        "        #     (4): Sequential(\n",
        "        #       (0): Linear(in_features=153, out_features=16, bias=True)\n",
        "        #       (1): ReLU(inplace=True)\n",
        "        #       (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
        "        #       (3): Linear(in_features=16, out_features=1, bias=True)\n",
        "        #     )\n",
        "        #   )\n",
        "        # )\n",
        "\n",
        "    def forward(self, x, sc_types):            #x: 1*29*153, sc_types: 1*29\n",
        "        # stack inputs with a .view for easier processing\n",
        "        x, sc_types = x.view(-1, x.size(-1)), sc_types.view(-1)     # x: 29*153, sc_types: 29\n",
        "        mask = sc_types != self.PAD_VAL           # mask : 29\n",
        "        x, sc_types = x[mask], sc_types[mask]     # x: 29*153, sc_types: 29\n",
        "\n",
        "        x_ = self.preproc(x)           # x_: 29*256\n",
        "        x_types = torch.zeros_like(x)  # x_types: 29*256\n",
        "        for i in range(self.N_TYPES):  #\n",
        "            t_idx = sc_types == i      # t_idx: 29\n",
        "            if torch.any(t_idx):\n",
        "                x_types[t_idx] = self.types_net[i](x_[t_idx])    #\n",
        "            else:\n",
        "                x_types = x_types + 0.0 * self.types_net[i](x_)  # fake call\n",
        "            # (only necessary for distributed training - to make sure all processes have gradients for all parameters)\n",
        "        x = x + x_types       # x, x_types: 29*153\n",
        "        return self.contribs_net(x)     # return: 29*5\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"Molecule transformer with message passing.\"\"\"\n",
        "\n",
        "    def __init__(self, d_atom, d_bond, d_sc_pair, d_sc_mol, N=6, d_model=512,\n",
        "                 d_ff=2048, d_ff_contrib=128, h=8, dropout=0.1, kernel_sz=128,\n",
        "                 enn_args={}, ann_args={}):     # d_atom: 21, d_bond: 8, d_sc_mol: 25\n",
        "        super().__init__()\n",
        "        assert d_model % h == 0\n",
        "        self.d_model = d_model\n",
        "        c = copy.deepcopy\n",
        "        bond_mess = ENNMessage(d_model, d_bond, kernel_sz, enn_args, ann_args)   #\n",
        "        sc_mess = ENNMessage(d_model, d_sc_pair, kernel_sz, enn_args)\n",
        "        eucl_dist_attn = MultiHeadedEuclDistAttention(h, d_model)\n",
        "        graph_dist_attn = MultiHeadedGraphDistAttention(h, d_model)\n",
        "        self_attn = MultiHeadedSelfAttention(h, d_model, dropout)\n",
        "        ff = FullyConnectedNet(d_model, d_model, [d_ff], dropout=[dropout])\n",
        "\n",
        "        message_passing_layer = MessagePassingLayer(\n",
        "            d_model, bond_mess, sc_mess, dropout, N)\n",
        "        attending_layer = AttendingLayer(\n",
        "            d_model, c(eucl_dist_attn), c(graph_dist_attn), c(self_attn), c(ff),\n",
        "            dropout\n",
        "        )\n",
        "\n",
        "        self.projection = nn.Linear(d_atom, d_model)\n",
        "        self.encoder = Encoder(message_passing_layer, attending_layer, N)\n",
        "        self.write_head = MyCustomHead(\n",
        "            2 * d_model + d_sc_mol, d_ff, d_ff_contrib, norm=True)\n",
        "\n",
        "    def forward(self, atom_x, bond_x, sc_pair_x, sc_mol_x, eucl_dists,\n",
        "                graph_dists, angles, mask, bond_idx, sc_idx, angles_idx,\n",
        "                sc_types):   # atom_x: 1*14*21, bond_x: 1*14*8, sc_pair_x: 1*29*16, sc_mol_x: 1*29*25, eucl_dists: 1*14*14\n",
        "                             # graph_dists: 1*14*14, angles: 1*44, mask: 1*14*14, bond_idx: 1*14*2,\n",
        "                             # sc_idx: 1*29*2, angles_idx: 1*44, sc_types: 1*29\n",
        "        x = self.encoder(\n",
        "            self.projection(atom_x), bond_x, sc_pair_x, eucl_dists, graph_dists,\n",
        "            angles, mask, bond_idx, sc_idx, angles_idx\n",
        "        )                              # 1*14*64\n",
        "        # for each sc constant in the batch select and concat the relevant pairs\n",
        "        # of atom  states.\n",
        "        x = torch.cat(\n",
        "            [_gather_nodes(x, sc_idx[:, :, 0], self.d_model),\n",
        "             _gather_nodes(x, sc_idx[:, :, 1], self.d_model),\n",
        "             sc_mol_x], dim=-1\n",
        "        )                              # x: 1*29*153\n",
        "        return self.write_head(x, sc_types)     # return: 29*5\n",
        "\n",
        "\n",
        "\n",
        "# parse arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "print(\"Beginning\")\n",
        "parser.add_argument('--batch_size', type=int, default=1)\n",
        "parser.add_argument('--epochs', type=int, default=2)\n",
        "parser.add_argument('--lr', type=float, default=4e-5, help='learning rate')\n",
        "parser.add_argument('--d_model', type=int, default=64,\n",
        "                    help='dimenstion of node state vector')\n",
        "parser.add_argument('--N', type=int, default=1,\n",
        "                    help='number of encoding layers')\n",
        "parser.add_argument('--h', type=int, default=1,\n",
        "                    help='number of attention heads')\n",
        "parser.add_argument('--wd', type=float, default=1e-2, help='weight decay')\n",
        "parser.add_argument('--dropout', type=float, default=0.0)\n",
        "parser.add_argument('--start_epoch', type=int, default=0)\n",
        "parser.add_argument('--fold_id', type=int, default=1)\n",
        "parser.add_argument('--version', type=int, default=1)\n",
        "parser.add_argument('--local_rank', type=int)\n",
        "args = parser.parse_args()\n",
        "\n",
        "# check if distributed training is possible and set model description\n",
        "#distributed_train = torch.cuda.device_count() > 1\n",
        "model_str = f'mol_transformer_v{args.version}_fold{args.fold_id}'\n",
        "\n",
        "# import data\n",
        "train_df = pd.read_csv(C.PROC_DATA_PATH+'train_proc_df.csv', index_col=0)      # train_df: 4658147*36\n",
        "                                                                               # ['atom_index_0', 'atom_index_1', 'scalar_coupling_constant', 'type',\n",
        "#        'atom_0', 'atom_1', 'dist', 'normed_dist', 'dist_min_rad',\n",
        "#        'dist_electro_neg_adj', 'type_0', 'type_1', 'type_2', 'type_3',\n",
        "#        'type_4', 'type_5', 'type_6', 'type_7', 'diangle', 'cos_angle',\n",
        "#        'cos_angle0', 'cos_angle1', 'ave_bond_length', 'std_bond_length',\n",
        "#        'ave_atom_weight', 'num_atoms', 'num_C_atoms', 'num_F_atoms',\n",
        "#        'num_H_atoms', 'num_N_atoms', 'num_O_atoms', 'molecule_id', 'fc', 'sd',\n",
        "#        'pso', 'dso']\n",
        "test_df = pd.read_csv(C.PROC_DATA_PATH+'test_proc_df.csv', index_col=0)        # test_df: 2505542*31\n",
        "                                                                               # ['atom_index_0', 'atom_index_1', 'type', 'atom_0', 'atom_1', 'dist',\n",
        "#        'normed_dist', 'dist_min_rad', 'dist_electro_neg_adj', 'type_0',\n",
        "#        'type_1', 'type_2', 'type_3', 'type_4', 'type_5', 'type_6', 'type_7',\n",
        "#        'diangle', 'cos_angle', 'cos_angle0', 'cos_angle1', 'ave_bond_length',\n",
        "#        'std_bond_length', 'ave_atom_weight', 'num_atoms', 'num_C_atoms',\n",
        "#        'num_F_atoms', 'num_H_atoms', 'num_N_atoms', 'num_O_atoms',\n",
        "#        'molecule_id']\n",
        "atom_df = pd.read_csv(C.PROC_DATA_PATH+'atom_df.csv', index_col=0)             # atom_df: 2358657*22\n",
        "                                                                               # ['type_H', 'type_C', 'type_N', 'type_O', 'type_F', 'degree_1',\n",
        "#        'degree_2', 'degree_3', 'degree_4', 'degree_5', 'SP', 'SP2', 'SP3',\n",
        "#        'hybridization_unspecified', 'aromatic', 'formal_charge', 'atomic_num',\n",
        "#        'ave_bond_length', 'ave_neighbor_weight', 'donor', 'acceptor',\n",
        "#        'molecule_id']\n",
        "bond_df = pd.read_csv(C.PROC_DATA_PATH+'bond_df.csv', index_col=0)             # bond_df: 2439811*11\n",
        "                                                                               # ['single', 'double', 'triple', 'aromatic', 'conjugated', 'in_ring',\n",
        "#        'dist', 'normed_dist', 'idx_0', 'idx_1', 'molecule_id']\n",
        "angle_in_df = pd.read_csv(C.PROC_DATA_PATH+'angle_in_df.csv', index_col=0)     # angle_in_df: 6368267*3\n",
        "                                                                               # ['molecule_id', 'b_idx', 'cos_angle']\n",
        "angle_out_df = pd.read_csv(C.PROC_DATA_PATH+'angle_out_df.csv', index_col=0)   # angle_out_df: 2764727*3\n",
        "                                                                               # ['molecule_id', 'b_idx', 'cos_angle']\n",
        "graph_dist_df = pd.read_csv(\n",
        "    C.PROC_DATA_PATH+'graph_dist_df.csv', index_col=0, dtype=np.int32)         # graph_dist_df: 2358657*30\n",
        "                                                                               # ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
        "#        '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24',\n",
        "#        '25', '26', '27', '28', 'molecule_id']\n",
        "structures_df = pd.read_csv(\n",
        "    C.PROC_DATA_PATH+'structures_proc_df.csv', index_col=0)                    # structures_df: 2358657*7\n",
        "                                                                               # ['molecule_name', 'atom_index', 'atom', 'x', 'y', 'z', 'molecule_id']\n",
        "\n",
        "train_mol_ids = pd.read_csv(C.PROC_DATA_PATH+'train_idxs_8_fold_cv.csv',\n",
        "                            usecols=[0, args.fold_id], index_col=0\n",
        "                            ).dropna().astype(int).iloc[:,0]                   # train_mol_ids: 74378*8\n",
        "                                                                               # columns: ['0', '1', '2', '3', '4', '5', '6', '7']\n",
        "                                                                               # usecols=[0, args.fold_id]: 74378*1\n",
        "                                                                               # columns: '0' (args.fold_id=1)\n",
        "                                                                               # train_mol_ids: (74377,)\n",
        "val_mol_ids = pd.read_csv(C.PROC_DATA_PATH+'val_idxs_8_fold_cv.csv',\n",
        "                            usecols=[0, args.fold_id], index_col=0\n",
        "                            ).dropna().astype(int).iloc[:,0]                   # val_mol_ids: 10626*8\n",
        "                                                                               # columns: ['0', '1', '2', '3', '4', '5', '6', '7']\n",
        "                                                                               # usecols=[0, args.fold_id]: 10626*1\n",
        "                                                                               # columns: '0' (args.fold_id=1)\n",
        "                                                                               # val_mol_ids: (10626,)\n",
        "\n",
        "test_mol_ids = pd.Series(test_df['molecule_id'].unique())                      # test_mol_ids: (45772,)\n",
        "\n",
        "\n",
        "# scale features\n",
        "train_df, sc_feat_means, sc_feat_stds = scale_features(\n",
        "    train_df, C.SC_FEATS_TO_SCALE, train_mol_ids, return_mean_and_std=True)    # SC_FEATS_TO_SCALE = ['dist', 'dist_min_rad', 'dist_electro_neg_adj',\n",
        "                                                                               # 'num_atoms', 'num_C_atoms', 'num_F_atoms', 'num_H_atoms',\n",
        "                                                                               # 'num_N_atoms', 'num_O_atoms', 'ave_bond_length',\n",
        "                                                                               # 'std_bond_length', 'ave_atom_weight']\n",
        "\n",
        "                                                                               # train_df: 4658147*36\n",
        "                                                                               # sc_feat_means: (12,)\n",
        "                                                                               # sc_feat_stds: (12,)\n",
        "\n",
        "\n",
        "test_df = scale_features(test_df, C.SC_FEATS_TO_SCALE, means=sc_feat_means, stds=sc_feat_stds)      # test_df: 2505542*31\n",
        "atom_df = scale_features(atom_df, C.ATOM_FEATS_TO_SCALE, train_mol_ids)        # atom_df: 2358657*22\n",
        "bond_df = scale_features(bond_df, C.BOND_FEATS_TO_SCALE, train_mol_ids)        # bond_df: 2439811*11\n",
        "\n",
        "# group data by molecule id                           # <class 'pandas.core.groupby.groupby.DataFrameGroupBy'>\n",
        "gb_mol_sc = train_df.groupby('molecule_id')                                    # gb_mol_sc: 85003*36\n",
        "test_gb_mol_sc = test_df.groupby('molecule_id')                                # test_mol_sc: 45772*31\n",
        "gb_mol_atom = atom_df.groupby('molecule_id')                                   # gb_mol_atom: 130775*22\n",
        "gb_mol_bond = bond_df.groupby('molecule_id')                                   # gb_mol_bond: 130775*11\n",
        "gb_mol_struct = structures_df.groupby('molecule_id')                           # gb_mol_struct: 130775*7\n",
        "gb_mol_angle_in = angle_in_df.groupby('molecule_id')                           # gb_mol_angle_in: 130775*3\n",
        "gb_mol_angle_out = angle_out_df.groupby('molecule_id')                         # gb_mol_angle_out: 130771*3\n",
        "gb_mol_graph_dist = graph_dist_df.groupby('molecule_id')                       # gb_mol_graph_dist: 130775*30\n",
        "\n",
        "# create dataloaders and fastai DataBunch\n",
        "set_seed(100)\n",
        "train_ds = MoleculeDataset(\n",
        "    train_mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_bond, gb_mol_struct,\n",
        "    gb_mol_angle_in, gb_mol_angle_out, gb_mol_graph_dist\n",
        ")                                                                             # <class 'moldataset.MoleculeDataset'>\n",
        "                                                                              # train_ds: 74377\n",
        "val_ds = MoleculeDataset(\n",
        "    val_mol_ids, gb_mol_sc, gb_mol_atom, gb_mol_bond, gb_mol_struct,\n",
        "    gb_mol_angle_in, gb_mol_angle_out, gb_mol_graph_dist\n",
        ")                                                                             # val_ds: 10626\n",
        "test_ds = MoleculeDataset(\n",
        "    test_mol_ids, test_gb_mol_sc, gb_mol_atom, gb_mol_bond, gb_mol_struct,\n",
        "    gb_mol_angle_in, gb_mol_angle_out, gb_mol_graph_dist\n",
        ")                                                                             # test_ds: 45772\n",
        "\n",
        "train_dl = DataLoader(train_ds, args.batch_size, shuffle=True, num_workers=8)    # <class 'torch.utils.data.dataloader.DataLoader'>\n",
        "                                                                                 # train_dl: 74377\n",
        "val_dl = DataLoader(val_ds, args.batch_size, num_workers=8)                      # val_dl: 10626\n",
        "test_dl = DeviceDataLoader.create(\n",
        "    test_ds, args.batch_size, num_workers=8,\n",
        "    collate_fn=partial(collate_parallel_fn, test=True), device=device)           # test_df: 45772\n",
        "\n",
        "db = DataBunch(train_dl, val_dl, collate_fn=collate_parallel_fn, device=device)  # <class 'fastai.basic_data.DataBunch'>\n",
        "db.test_dl = test_dl\n",
        "\n",
        "# set up model\n",
        "set_seed(100)\n",
        "d_model = args.d_model                                                           # d_model: 64\n",
        "enn_args = dict(layers=3*[d_model], dropout=3*[0.0], layer_norm=True)  # {'layers': [64, 64, 64], 'dropout': [0.0, 0.0, 0.0], 'layer_norm': True}\n",
        "ann_args = dict(layers=1*[d_model], dropout=1*[0.0], layer_norm=True,\n",
        "                out_act=nn.Tanh())           # {'layers': [64], 'dropout': [0.0], 'layer_norm': True, 'out_act': Tanh()}\n",
        "model = Transformer(\n",
        "    C.N_ATOM_FEATURES, C.N_BOND_FEATURES, C.N_SC_EDGE_FEATURES,\n",
        "    C.N_SC_MOL_FEATURES, N=args.N, d_model=d_model, d_ff=d_model*4,\n",
        "    d_ff_contrib=d_model//4, h=args.h, dropout=args.dropout,\n",
        "    kernel_sz=min(128, d_model), enn_args=enn_args, ann_args=ann_args)  # C.N_ATOM_FEATURES: 21, C.N_BOND_FEATURES: 8,\n",
        "                                                                        # C.N_SC_EDGE_FEATURES: 16, C.N_SC_MOL_FEATURES: 25\n",
        "                                                                        # args.N: 1, d_model: 64, args.h: 1, args.dropout: 0.0\n",
        "\n",
        "# model cuda\n",
        "#model = nn.DataParallel(model,device_ids=[3])\n",
        "#model = model.to(device)\n",
        "\n",
        "# initialize distributed\n",
        "''''if distributed_train:\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl', init_method='env://')'''\n",
        "\n",
        "# train model\n",
        "callback_fns = [\n",
        "    partial(GradientClipping, clip=10), GroupMeanLogMAE,\n",
        "    partial(SaveModelCallback, every='improvement', mode='min',\n",
        "            monitor='group_mean_log_mae', name=model_str)      # <class 'callbacks.GradientClipping'>\n",
        "]\n",
        "learn = Learner(db, model, metrics=[rmse, mae], callback_fns=callback_fns,\n",
        "                wd=args.wd, loss_func=contribs_rmse_loss)      # args.wd: 0.01\n",
        "\n",
        "if args.start_epoch > 0:\n",
        "    learn.load(model_str, device=device)                      # model_str: mol_transformer_v1_fold1\n",
        "    torch.to(device).empty_cache()\n",
        "#if distributed_train: learn = learn.to_distributed(args.local_rank)\n",
        "\n",
        "learn.fit_one_cycle(args.epochs, max_lr=args.lr, start_epoch=args.start_epoch)   #\n",
        "\n",
        "\n",
        "# make predictions\n",
        "val_contrib_preds = learn.get_preds(DatasetType.Valid)\n",
        "test_contrib_preds = learn.get_preds(DatasetType.Test)\n",
        "val_preds = val_contrib_preds[0][:,-1].detach().numpy() * C.SC_STD + C.SC_MEAN\n",
        "test_preds = test_contrib_preds[0][:,-1].detach().numpy() * C.SC_STD + C.SC_MEAN\n",
        "\n",
        "\n",
        "# store results\n",
        "store_submit(test_preds, model_str, print_head=True)\n",
        "store_oof(val_preds, model_str, print_head=True)"
      ],
      "metadata": {
        "id": "806NlbVKrbUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xf9KSa6TrNqF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}